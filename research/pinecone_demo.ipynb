{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "load_dotenv()\n",
    "\n",
    "index_name = os.environ['INDEX_NAME']\n",
    "pinecone_api =os.environ['PINECONE_API_KEY']\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=pinecone_api,  # find at app.pinecone.io\n",
    "    environment=os.environ['PINECONE_ENV'] # next to api key in console\n",
    ")\n",
    "\n",
    "     \n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Replace consecutive spaces, newlines and tabs\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    # create a loader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    # load your data\n",
    "    data = loader.load()\n",
    "    # Split your data up into smaller documents with Chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    documents = text_splitter.split_documents(data)\n",
    "    print(\"DOCUMENTS================\",documents)\n",
    "    print(\"LENGHT================\",len(documents))\n",
    "    page_content =[t.page_content  for t in documents]\n",
    "    return documents,page_content\n",
    "\n",
    "\n",
    "\n",
    "llm = GooglePalm(google_api_key=os.environ[\"GOOGLE_PALM_API_KEY\"], temperature=0.1)\n",
    "\n",
    "# Define a function to create embeddings\n",
    "def create_embeddings():\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    embeddings = HuggingFaceBgeEmbeddings(model_name=model)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# # Define a function to upsert embeddings to Pinecone\n",
    "def upsert_embeddings_to_pinecone(index, embeddings,page_content_list):\n",
    "    i= Pinecone.from_texts(page_content_list,embedding=embeddings,index_name=index)\n",
    "    return i\n",
    "\n",
    "\n",
    "#================== RETRIEVE QUERY ===============================\n",
    "\n",
    "## Cosine Similarity Retreive Results from VectorDB\n",
    "def retrieve_query(index_name,query,k=20):\n",
    "    # matching_results=index.similarity_search(query,k=k)\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    emb = model.encode(query)\n",
    "    index = pinecone.Index(index_name=index_name)\n",
    "    res = index.query(emb.tolist(),top_k=k,include_metadata=True)\n",
    "    print(res['matches'])\n",
    "    return res['matches']\n",
    "\n",
    "# # Define the function for similarity search\n",
    "# def similarity_search(query_embeddings):\n",
    "#     pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "#     index_name = os.environ['INDEX_NAME']\n",
    "\n",
    "#     index = pc.Index(index_name)\n",
    "#     response = index.query(vector=query_embeddings, top_k=2,include_values=False)\n",
    "#     return response['matches']\n",
    "\n",
    "\n",
    "# Define the function to retrieve relevant documents based on similarity search results\n",
    "def retrieve_relevant_documents(similarity_results):\n",
    "    relevant_documents = []\n",
    "    for match in similarity_results:\n",
    "        if match['score'] >= 0.2:  # Adjust the score threshold as needed\n",
    "            relevant_documents.append(match['metadata']['text'])\n",
    "    print(\"RELEVANT DOCUMENTS =====================\" ,relevant_documents)\n",
    "    return relevant_documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # return matching_results\n",
    "\n",
    "\n",
    "llm = GooglePalm(google_api_key=os.environ['GOOGLE_PALM_API_KEY'], temperature=0.1)  # Adjust temperature as needed\n",
    "\n",
    "# Define the function to get response from Google Palm LLM\n",
    "def get_llm_response(query_text, relevant_documents):\n",
    "    llm = GooglePalm(google_api_key=os.environ['GOOGLE_PALM_API_KEY'], temperature=0.1)  # Adjust temperature as needed\n",
    "    prompt = \"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
    "    Do not answer anything apart from the given context, you will only answer queries regarding the resume of the candidate provided to you\n",
    "\n",
    "    Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
    "    However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
    "    strike a friendly and converstional tone. \n",
    "\n",
    "    If the passage is irrelevant to the answer, you may ignore it.\n",
    "    QUESTION: '{query_text}'\n",
    "    PASSAGE: '{relevant_documents}'\"\"\"\n",
    "    prompt = f\"Query: {query_text}\\nContext: {relevant_documents}\"\n",
    "    response=llm.generate(prompts=[prompt])\n",
    "    return response\n",
    "\n",
    "query = \"How many years of work experience she has?\"\n",
    "similarity_results=retrieve_query(index_name,query)\n",
    "\n",
    "doc_search = retrieve_relevant_documents(similarity_results)\n",
    "print(\"=========================QUERY EMBEDDINGS RETREIVED ===============================================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "answer = get_llm_response(query_text=query,relevant_documents=doc_search)\n",
    "print(answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def input_pdf_text(uploaded_file):\n",
    "    reader=pdf.PdfReader(uploaded_file)\n",
    "    text=\"\"\n",
    "    for page in range(len(reader.pages)):\n",
    "        page=reader.pages[page]\n",
    "        text+=str(page.extract_text())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# Load the SentenceTransformer model for encoding text into embeddings\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Define the function for similarity search\n",
    "def similarity_search(query_embeddings):\n",
    "    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "    index_name = os.environ['INDEX_NAME']\n",
    "\n",
    "    index = pc.Index(index_name)\n",
    "    response = index.query(vector=query_embeddings, top_k=2,include_values=False)\n",
    "    return response['matches']\n",
    "\n",
    "\n",
    "# Define the function to retrieve relevant documents based on similarity search results\n",
    "def retrieve_relevant_documents(similarity_results):\n",
    "    relevant_documents = []\n",
    "    for match in similarity_results:\n",
    "        if match['score'] >= 0.2:  # Adjust the score threshold as needed\n",
    "            relevant_documents.append(match['text'])\n",
    "    print(\"RELEVANT DOCUMENTS =====================\" ,relevant_documents)\n",
    "    return relevant_documents\n",
    "\n",
    "# Define the function to get response from Google Palm LLM\n",
    "def get_llm_response(query_text, relevant_documents):\n",
    "    llm = GooglePalm(google_api_key=os.environ['GOOGLE_PALM_API_KEY'], temperature=0.1)  # Adjust temperature as needed\n",
    "    prompt = \"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \\\n",
    "    Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \\\n",
    "    However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \\\n",
    "    strike a friendly and converstional tone. \\\n",
    "    If the passage is irrelevant to the answer, you may ignore it.\n",
    "    QUESTION: '{query_text}'\n",
    "    PASSAGE: '{relevant_documents}'\"\"\"\n",
    "    prompt = f\"Query: {query_text}\\nContext: {relevant_documents}\"\n",
    "\n",
    "    response=llm.generate(prompts=[prompt])\n",
    "\n",
    "    return response\n",
    "\n",
    "  # Format relevant documents properly\n",
    "    # input_documents = [{\"text_\" + str(i+1):doc} for i,doc in enumerate(relevant_documents)]\n",
    "\n",
    "    # print(\"input_doc\",input_documents)\n",
    "    # chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    # response = chain.run(question=query_text, input_documents=re)\n",
    "# Function to process user query\n",
    "def process_user_query(query_text):\n",
    "    # Convert query text to embeddings\n",
    "    query_embeddings = model.encode(sentences=query_text)\n",
    "    print(\"Converted query text to embeddings\")\n",
    "\n",
    "    # Perform similarity search\n",
    "    similarity_results = similarity_search(query_embeddings.tolist())\n",
    "    print(\"Performed similarity search\")\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    relevant_documents = retrieve_relevant_documents(similarity_results)\n",
    "    print(\"Retrieved relevant documents\")\n",
    "    \n",
    "    # Generate prompt with context\n",
    "    # context_text = \" \".join([str(doc) for doc in context_text])\n",
    "    # print(\"================CONTEXT TEXT \",  context_text)\n",
    "    # Get response from LLM\n",
    "    response = get_llm_response(query_text,relevant_documents=relevant_documents)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the function with user query\n",
    "user_query =\"who is manmeet kaur\"\n",
    "response = process_user_query(user_query)\n",
    "print(\"LLM Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
